{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e773098-3831-4aef-bccd-66f62025cdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiger\\AppData\\Local\\Temp\\ipykernel_26016\\438646873.py:33: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  inputdf=pd.read_csv(\"AD_2023_processed.csv\")\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "inputdf=pd.read_csv(\"AD_2023_processed.csv\")\n",
    "wavenumbermin=900\n",
    "wavenumbermax=2000\n",
    "wavenumberpoints=701\n",
    "wavenumber=[float(i) for i in inputdf.columns[10:].values]\n",
    "#wavenumbermax=900+241\n",
    "#wavenumberpoints=242\n",
    "\n",
    "#\"Asymmetric Least Squares Smoothing\" by P. Eilers and H. Boelens in 2005\n",
    "def baseline_als(y, lam=10000, p=0.0001, niter=10):\n",
    "    L = len(y)\n",
    "    D = sparse.diags([1,-2,1],[0,-1,-2], shape=(L,L-2))\n",
    "    D = lam * D.dot(D.transpose()) # Precompute this term since it does not depend on `w`\n",
    "    w = np.ones(L)\n",
    "    W = sparse.spdiags(w, 0, L, L)\n",
    "    for i in range(niter):\n",
    "        W.setdiag(w) # Do not create a new matrix, just update diagonal values\n",
    "        Z = W + D\n",
    "        z = spsolve(Z, w*y)\n",
    "        w = p * (y > z) + (1-p) * (y < z)\n",
    "    return z\n",
    "\n",
    "def moremetrics(trainner, X, y,classes):\n",
    "    #print precision recall f1\n",
    "    scores = cross_validate(trainner, X, y, cv=5,scoring=('precision','recall','f1'))\n",
    "    print(\"validation precision: %\", float(sum(scores['test_precision'])/len(scores['test_precision'])*100))\n",
    "    print(\"validation recall: %\", float(sum(scores['test_recall'])/len(scores['test_recall'])*100))\n",
    "    print(\"validation f1: %\", float(sum(scores['test_f1'])/len(scores['test_f1'])*100))\n",
    "    #print(confusion_matrix(y_test,y_pred))\n",
    "    #print(classification_report(y_test,y_pred))\n",
    "    #confusion matrix\n",
    "    return classes+[float(sum(scores['test_precision'])/len(scores['test_precision'])*100),\\\n",
    "            float(sum(scores['test_recall'])/len(scores['test_recall'])*100),\\\n",
    "            float(sum(scores['test_f1'])/len(scores['test_f1'])*100)]\n",
    "\n",
    "def f_importances(coef, intercept, names,classes,svm_C,svmcrossacc,i):\n",
    "    with open('./outputs/svm_feature_importance.csv','a', newline='',encoding='utf-8') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile)\n",
    "        if i ==0:\n",
    "            spamwriter.writerow([i]+['class']*len(classes)+['crossacc']\n",
    "                                +['class']*len(classes)+['C']+['intercept']+names)\n",
    "        coef=coef.tolist()\n",
    "        spamwriter.writerows([[i]+classes+svmcrossacc+[svm_C]+[str(i) for i in intercept.tolist()]+[str(k) for k in j] for j in coef])\n",
    "    plt.figure()\n",
    "    plt.plot(names,coef[0])\n",
    "    plt.ylabel('svm_feature importance')\n",
    "\n",
    "def doUMAP(column,classes,X,y):\n",
    "\n",
    "    #plot UMAP plot plt.figure(1)\n",
    "        \n",
    "    X=StandardScaler().fit_transform(X)\n",
    "\n",
    "    y = y[column].values.tolist()\n",
    "    \n",
    "    import umap\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    \n",
    "    reducer = umap.UMAP(a=None, angular_rp_forest=False, b=None,\n",
    "     force_approximation_algorithm=False, init='spectral', learning_rate=1.0,\n",
    "     local_connectivity=1.0, low_memory=False, metric='euclidean',\n",
    "     metric_kwds=None, min_dist=0.1, n_components=2, n_epochs=None,\n",
    "     n_neighbors=15, negative_sample_rate=5, output_metric='euclidean',\n",
    "     output_metric_kwds=None, random_state=42, repulsion_strength=1.0,\n",
    "     set_op_mix_ratio=1.0, spread=1.0, target_metric='categorical',\n",
    "     target_metric_kwds=None, target_n_neighbors=-1, target_weight=0.5,\n",
    "     transform_queue_size=4.0, transform_seed=42, unique=False, verbose=False)\n",
    "\n",
    "    reducer.fit(X)\n",
    "    embedding = reducer.transform(X)\n",
    "    \n",
    "  \n",
    "    target={'target':y}\n",
    "    index={'index':[]}\n",
    "    target = pd.DataFrame(data=target)\n",
    "    index=pd.DataFrame(data=index)\n",
    "\n",
    "    principalDf = pd.DataFrame(data = embedding, columns = ['Embedding 1', 'Embedding 2'])\n",
    "    finalDf = pd.concat([principalDf, target], axis=1)\n",
    "\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    fig.suptitle('UMAP')\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('Embedding 1', fontsize = 15)\n",
    "    ax.set_ylabel('Embedding 2', fontsize = 15)\n",
    "    ax.set_title('2 component UMAP', fontsize = 20)\n",
    "    targets=classes\n",
    "    colors = ['C0','C1','C2','C3','C4','C5','C6','C7','C8',\n",
    "              'C9','C10','C11','C12','C13']\n",
    "    for target, color in zip(targets,colors):\n",
    "        indicesToKeep = finalDf['target'] == target\n",
    "        ax.scatter(finalDf.loc[indicesToKeep, 'Embedding 1']\n",
    "                   , finalDf.loc[indicesToKeep, 'Embedding 2']\n",
    "                   , c = color\n",
    "                   , s = 10)\n",
    "    ax.legend(targets)\n",
    "    ax.grid()\n",
    "    #plt.xlim(-50, 50)\n",
    "    #plt.ylim(-50, 50)\n",
    "    #save UMAP figure\n",
    "    plt.savefig(\"./outputs/\"+column+\"UMAP.png\")\n",
    "\n",
    "def doPCA(filename,column,classes,X,y):\n",
    "    #exclude grahene peak\n",
    "    #wavenumber=wavenumber[:1233]+wavenumber[1355:]\n",
    "           \n",
    "\n",
    "    y = y[column].values.tolist()\n",
    "\n",
    "    #plot PCA plot plt.figure(1)\n",
    "        \n",
    "    #X=StandardScaler().fit_transform(X)\n",
    "    \n",
    "    target={'target':y}\n",
    "    index={'index':[]}\n",
    "    target = pd.DataFrame(data=target)\n",
    "    index=pd.DataFrame(data=index)\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(X)[:,0:3]\n",
    "    pca.components_[0]\n",
    "    principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "    finalDf = pd.concat([principalDf, target], axis=1)\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    fig.suptitle('PCA')\n",
    "    ax = fig.add_subplot(1,1,1,projection='3d') \n",
    "    ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "    ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "    ax.set_zlabel('Principal Component 3', fontsize = 15)\n",
    "    ax.set_title('2 component PCA', fontsize = 20)\n",
    "    targets=classes\n",
    "    colors = ['C0','C1','C2','C3','C4','C5','C6','C7','C8',\n",
    "              'C9','C10','C11','C12','C13']\n",
    "    for target, color in zip(targets,colors):\n",
    "        indicesToKeep = finalDf['target'] == target\n",
    "        #ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "        #           , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "        #           , c = color\n",
    "        #           , s = 10)\n",
    "        ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1'],\n",
    "                   finalDf.loc[indicesToKeep, 'principal component 2'],\n",
    "                   finalDf.loc[indicesToKeep, 'principal component 3'],\n",
    "                   c = color,\n",
    "                   s = 10)\n",
    "    ax.legend(targets)\n",
    "    ax.grid()\n",
    "    #plt.xlim(-50, 50)\n",
    "    #plt.ylim(-50, 50)\n",
    "    print('pc1 pc2 explains',pca.explained_variance_ratio_)\n",
    "    #save PCA figure\n",
    "    plt.savefig(\"./outputs/\"+filename+column+\"PCA.png\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(wavenumber,pca.components_[0], label = \"PC1\")\n",
    "    plt.plot(wavenumber,pca.components_[1], label = \"PC2\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./outputs/\"+filename+column+\"PC.png\")\n",
    "\n",
    "    csv_output=np.zeros((len(targets)+1,len(wavenumber)))\n",
    "    csv_output[0,:]=wavenumber\n",
    "    plt.figure()\n",
    "    shift=0\n",
    "    for target, color in zip(targets,colors):\n",
    "        plottarget=finalDf[finalDf['target']==target]\n",
    "        plotX=X[finalDf['target']==target,:]\n",
    "        plt.plot(wavenumber,np.mean(plotX,axis=0)+shift,c=color,label=target)\n",
    "        shift+=1\n",
    "        csv_output[shift,:]=np.mean(plotX,axis=0)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./outputs/\"+filename+column+\"spectra.png\")\n",
    "    np.savetxt('./outputs/'+filename+column+\"spectra.csv\", csv_output, delimiter=',')\n",
    "    \n",
    "    \n",
    "def doML(MLdf,column,classes,standardize=False,moremetrices=True,backgroundsub=False,svm_C=1e-10,max_iter=400,normalizepeak=475):\n",
    "    #exclude grahene peak\n",
    "    #wavenumber=wavenumber[:1233]+wavenumber[1355:]\n",
    "    X=[]\n",
    "    y=[]\n",
    "    folder=[]\n",
    "    outaccuracy=[]\n",
    "    \n",
    "    outputname=\"_\".join([str(i) for i in classes])\n",
    "\n",
    "\n",
    "    df=MLdf.loc[MLdf[column].isin(classes)]\n",
    "    if MLdf.loc[MLdf[column].isin(classes)].size==0:\n",
    "        print(substrate+process+'not found')\n",
    "        return\n",
    "      \n",
    "    X = np.array(df.iloc[:,10:])\n",
    "    def reject_outliers(data,y, m=3):\n",
    "        maximums=np.max(data,axis=1)\n",
    "        index=abs(maximums - np.mean(maximums)) < m * np.std(maximums)\n",
    "        return data[index,:],y[index]\n",
    "    X,df=reject_outliers(X,df)\n",
    "    X=savgol_filter(X, 7, 2)\n",
    "\n",
    "    rowtodelete=np.max(X[:,normalizepeak-5:normalizepeak+5],axis=1)>0.1 #not dry samples\n",
    "    #rowtodelete=np.logical_and(np.max(X[:,normalizepeak-5:normalizepeak+5],axis=1)>0.1,\n",
    "    #                           np.max(X,axis=1)<=2*np.max(X[:,normalizepeak-5:normalizepeak+5],axis=1))\n",
    "\n",
    "    X=X[rowtodelete,:]\n",
    "    df=df.iloc[rowtodelete]\n",
    "\n",
    "    for i in range(np.shape(X)[0]):\n",
    "        #normalize 1661\n",
    "        peak=max(X[i,normalizepeak-5:normalizepeak+5])\n",
    "        if peak !=0:\n",
    "            X[i,:]=X[i,:]/peak\n",
    "\n",
    "    codes, uniques=pd.factorize(df[column])\n",
    "    y = np.array(codes)\n",
    "    classes=uniques.tolist()\n",
    "\n",
    "    if backgroundsub:\n",
    "        for i in range(np.shape(X)[0]):\n",
    "            X[i,:]=X[i,:]-baseline_als(X[i,:])\n",
    "\n",
    "    X_trainori, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "    #standardize\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_trainori)\n",
    "        X_train=scaler.transform(X_trainori)\n",
    "        X_test=scaler.transform(X_test)\n",
    "        #X[~y.astype(bool)]=StandardScaler().fit_transform(X[~y.astype(bool)])\n",
    "        #X[y.astype(bool)]=StandardScaler().fit_transform(X[y.astype(bool)])\n",
    "        #X=normalize(X, norm='l2')\n",
    "    else:\n",
    "        X_train=X_trainori\n",
    "\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "    strat_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=12138)\n",
    "\n",
    "    crossscaler = StandardScaler()\n",
    "    #train svm on raw data\n",
    "    print(\"svm train on raw\",len(y_train),\"test\",len(y_test))\n",
    "    svmtrainner = svm.LinearSVC(multi_class='ovr',C=svm_C,max_iter=max_iter)\n",
    "    if standardize: \n",
    "        scores = cross_val_score(Pipeline([('transformer', crossscaler), ('estimator', svmtrainner)]), X, y, cv=strat_k_fold)\n",
    "    else:\n",
    "        scores = cross_val_score(Pipeline([('estimator', svmtrainner)]), X, y, cv=strat_k_fold)\n",
    "    svmtrainner.fit(X_train, y_train)\n",
    "    y_pred = svmtrainner.predict(X_test)\n",
    "    svmcrossacc=sum(scores)/len(scores)*100\n",
    "    print(\"validation scores: %\", float(sum(scores)/len(scores)*100), scores)\n",
    "    if moremetrices:\n",
    "        outaccuracy.append(moremetrics(svmtrainner, X, y,classes))\n",
    "\n",
    "    #train logistics regression on raw data\n",
    "    print(\"logistics regression train on raw\",len(y_train),\"test\",len(y_test))\n",
    "##    parameters = {'estimator__C':[1e-1,1e-2,1e-3,1e-4],\n",
    "##                  'estimator__l1_ratio':[0.1,0.2,0.3,0.4,0.5,0.6]}\n",
    "    parameters = {'estimator__C':[svm_C],\n",
    "                  'estimator__l1_ratio':[0.3]}\n",
    "    logisticstrainner = LogisticRegression(solver='saga',penalty='elasticnet',max_iter=max_iter)\n",
    "    if standardize:    \n",
    "        clf = GridSearchCV(Pipeline([('transformer', crossscaler), ('estimator', logisticstrainner)]), parameters)\n",
    "    else:\n",
    "        clf = GridSearchCV(Pipeline([('estimator', logisticstrainner)]), parameters)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(clf.best_params_)\n",
    "    print(clf.score(X_test, y_test))\n",
    "    logisticstrainner = LogisticRegression(solver='saga',max_iter=max_iter,C=clf.best_params_['estimator__C']\n",
    "                                           ,penalty='elasticnet',l1_ratio=clf.best_params_['estimator__l1_ratio'])\n",
    "    if standardize: \n",
    "        scores = cross_val_score(Pipeline([('transformer', crossscaler), ('estimator', logisticstrainner)]), X, y, cv=strat_k_fold)\n",
    "    else:\n",
    "        scores = cross_val_score(Pipeline([('estimator', logisticstrainner)]), X, y, cv=strat_k_fold)        \n",
    "    logisticstrainner.fit(X_train, y_train)\n",
    "    y_pred = logisticstrainner.predict(X_test)\n",
    "    logcrossacc=sum(scores)/len(scores)*100\n",
    "    print(\"validation scores: %\", float(sum(scores)/len(scores)*100), scores)\n",
    "    if moremetrices:\n",
    "        outaccuracy.append(moremetrics(logisticstrainner, X, y,classes))\n",
    "\n",
    "    #train Decision tree on raw data\n",
    "    print(\"Decision tree train on raw\",len(y_train),\"test\",len(y_test))\n",
    "    decisiontrainner = DecisionTreeClassifier()\n",
    "    if standardize:\n",
    "        scores = cross_val_score(Pipeline([('transformer', crossscaler), ('estimator', decisiontrainner)]), X, y, cv=strat_k_fold)\n",
    "    else:\n",
    "        scores = cross_val_score(Pipeline([('estimator', decisiontrainner)]), X, y, cv=strat_k_fold)        \n",
    "    decisiontrainner.fit(X_train, y_train)\n",
    "    y_pred = decisiontrainner.predict(X_test)\n",
    "    print(\"validation scores: %\", float(sum(scores)/len(scores)*100), scores)\n",
    "    if moremetrices:\n",
    "        outaccuracy.append(moremetrics(decisiontrainner, X, y,classes))\n",
    "    \n",
    "    #train Random forest on raw data\n",
    "    print(\"Random forest train on raw\",len(y_train),\"test\",len(y_test))\n",
    "    randomforesttrainner = RandomForestClassifier(n_estimators=100)\n",
    "    if standardize:\n",
    "        scores = cross_val_score(Pipeline([('transformer', crossscaler), ('estimator', randomforesttrainner)]), X, y, cv=strat_k_fold)\n",
    "    else:\n",
    "        scores = cross_val_score(Pipeline([('estimator', randomforesttrainner)]), X, y, cv=strat_k_fold)        \n",
    "    randomforesttrainner.fit(X_train, y_train)\n",
    "    y_pred = randomforesttrainner.predict(X_test)\n",
    "    print(\"validation scores: %\", float(sum(scores)/len(scores)*100), scores)\n",
    "    if moremetrices:\n",
    "        outaccuracy.append(moremetrics(randomforesttrainner, X, y,classes))\n",
    "\n",
    "    #XGBoost\n",
    "    from xgboost import XGBClassifier\n",
    "    print(\"XGBoost train on raw\",len(y_train),\"test\",len(y_test))\n",
    "    XGBoosttrainner = XGBClassifier()\n",
    "    if standardize:\n",
    "        scores = cross_val_score(Pipeline([('transformer', crossscaler), ('estimator', XGBoosttrainner)]), X, y, cv=strat_k_fold)\n",
    "    else:\n",
    "        scores = cross_val_score(Pipeline([('estimator', XGBoosttrainner)]), X, y, cv=strat_k_fold)        \n",
    "    XGBoosttrainner.fit(X_train, y_train)\n",
    "    y_pred = XGBoosttrainner.predict(X_test)\n",
    "    print(\"validation scores: %\", float(sum(scores)/len(scores)*100), scores)\n",
    "    if moremetrices:\n",
    "        outaccuracy.append(moremetrics(XGBoosttrainner, X, y,classes))\n",
    "\n",
    "    #non-negative sparse logistic regression\n",
    "##    from glmnet import LogitNet\n",
    "##    print(\"non-negative logistic train on raw\",len(y_train),\"test\",len(y_test))\n",
    "##    breakpoint()\n",
    "##    Logittrainner = LogitNet()\n",
    "##    scores = cross_val_score(Pipeline([('transformer', crossscaler), ('estimator', Logittrainner)]), X, y, cv=5)\n",
    "##    Logittrainner.fit(X_train, y_train)\n",
    "##    y_pred = Logittrainner.predict(X_test)\n",
    "##    Logitcrossacc=sum(scores)/len(scores)*100\n",
    "##    print(\"validation scores: %\", float(sum(scores)/len(scores)*100), scores)\n",
    "##    if moremetrices:\n",
    "##        outaccuracy.append(moremetrics(Logittrainner, X, y,classes))\n",
    "        \n",
    "    #plot PCA plot plt.figure(1)\n",
    "    target={'target':y}\n",
    "    index={'index':[]}\n",
    "    target = pd.DataFrame(data=target)\n",
    "    index=pd.DataFrame(data=index)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    principalComponents = pca.fit_transform(X)\n",
    "    principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'])\n",
    "    finalDf = pd.concat([principalDf, target], axis=1)\n",
    "    finalPC_Df = pd.DataFrame(data = pca.components_[0:2,:], columns = wavenumber)\n",
    "\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    fig.suptitle('PCA')\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "    ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "    ax.set_title('2 component PCA', fontsize = 20)\n",
    "    targets = [0,1]\n",
    "    targetsname=\" \".join([str(i) for i in classes])\n",
    "    colors = ['g','r','b']\n",
    "    for target, color in zip(targets,colors):\n",
    "        indicesToKeep = finalDf['target'] == target\n",
    "        ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "                   , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "                   , c = color\n",
    "                   , s = 10)\n",
    "    ax.legend(targets)\n",
    "    ax.grid()\n",
    "    #plt.xlim(-50, 50)\n",
    "    #plt.ylim(-50, 50)\n",
    "    print('pc1 pc2 explains',pca.explained_variance_ratio_)\n",
    "    #save PCA csv\n",
    "    plt.savefig(\"./outputs/\"+outputname+\"PCA.png\")\n",
    "    finalDf.to_csv(\"./outputs/\"+outputname+\"PCA.csv\")\n",
    "    finalPC_Df.to_csv(\"./outputs/\"+outputname+\"PCA_PC.csv\")\n",
    "    \n",
    "    plt.close(\"all\")\n",
    "    for target, color in zip(targets,colors):\n",
    "        plottarget=y[y==target]\n",
    "        plotX=X[y==target,:]\n",
    "        plotindex=np.random.permutation(plotX.shape[0])\n",
    "        plt.plot(wavenumber,plotX[plotindex[0],:],c=color,label=target)\n",
    "        for i in range(20):\n",
    "            plt.plot(wavenumber,plotX[plotindex[i],:],c=color)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"./outputs/\"+outputname+\"spectra.png\")\n",
    "    \n",
    "    #plot fig\n",
    "        \n",
    "    #output svm feature importance for 5 times\n",
    "    for i in range(5):\n",
    "        X_trainori, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "        #standardize\n",
    "        if standardize:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_trainori)\n",
    "            X_train=scaler.transform(X_trainori)\n",
    "            X_test=scaler.transform(X_test)\n",
    "        else:\n",
    "            X_train=X_trainori\n",
    "        svmtrainner = svm.LinearSVC(multi_class='ovr',fit_intercept=True,max_iter=max_iter)#,penalty='l1',dual=False)\n",
    "        svmtrainner.fit(X_train, y_train)\n",
    "        y_pred = svmtrainner.predict(X_test)\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        accmatrix = confusion_matrix(y_test, y_pred)\n",
    "        accbyclass=accmatrix.diagonal()/accmatrix.sum(axis=1)\n",
    "        #print(svmtrainner.coef_,features_names)\n",
    "        print('svm interscept is:',svmtrainner.intercept_)\n",
    "        f_importances(svmtrainner.coef_, svmtrainner.intercept_, wavenumber,classes,svm_C,[svmcrossacc]+accbyclass.tolist(),i)\n",
    "    #output logistic feature importance for 5 times\n",
    "    for i in range(5):\n",
    "        X_trainori, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "        #standardize\n",
    "        if standardize:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(X_trainori)\n",
    "            X_train=scaler.transform(X_trainori)\n",
    "            X_test=scaler.transform(X_test)\n",
    "        else:\n",
    "            X_train=X_trainori\n",
    "        logisticstrainner = LogisticRegression(solver='saga',max_iter=max_iter,C=clf.best_params_['estimator__C']\n",
    "                                               ,penalty='elasticnet',l1_ratio=clf.best_params_['estimator__l1_ratio'],fit_intercept=True)\n",
    "        logisticstrainner.fit(X_train, y_train)\n",
    "        y_pred = logisticstrainner.predict(X_test)\n",
    "        print(classification_report(y_test,y_pred))\n",
    "        accmatrix = confusion_matrix(y_test, y_pred)\n",
    "        accbyclass=accmatrix.diagonal()/accmatrix.sum(axis=1)\n",
    "        #print(svmtrainner.coef_,features_names)\n",
    "        print('log interscept is:',logisticstrainner.intercept_)\n",
    "        f_importances(logisticstrainner.coef_,logisticstrainner.intercept_, wavenumber,classes,str(clf.best_params_),[logcrossacc]+accbyclass.tolist(),i)\n",
    "\n",
    "        \n",
    "    plt.close(\"all\")\n",
    "    #plt.plot(wavenumber,svmtrainner.coef_[0])\n",
    "    #plt.show()\n",
    "    if moremetrices:\n",
    "        with open('./summary.csv','a', newline='',encoding='utf-8') as csvfile:\n",
    "            spamwriter = csv.writer(csvfile)\n",
    "            spamwriter.writerows(outaccuracy)\n",
    "    #plt.show()\n",
    "\n",
    "def doCC(CCdf,outputname='CCfigure',backgroundsub=False):\n",
    "    \n",
    "\n",
    "    X=[]\n",
    "\n",
    "    folder=[]\n",
    "    outaccuracy=[]\n",
    "\n",
    "    X = np.array(CCdf.iloc[:,10:])\n",
    "    if backgroundsub:\n",
    "        for i in range(np.shape(X)[0]):\n",
    "            X[i,:]=X[i,:]-baseline_als(X[i,:])\n",
    "    \n",
    "    pd.DataFrame(X).to_csv(\"./outputs/\"+outputname+\"_data.csv\")\n",
    "    \n",
    "    plt.plot(wavenumber,np.average(X,axis=0))\n",
    "    plt.xlim([wavenumbermin,wavenumbermax])\n",
    "    plt.savefig(\"./outputs/\"+outputname+\"_average.png\")\n",
    "    plt.close(\"all\")\n",
    "    X=StandardScaler().fit_transform(X)\n",
    "\n",
    "    #calculate correlation\n",
    "    cor=np.dot(np.transpose(X),X)/X.shape[0]\n",
    "    pd.DataFrame(cor).to_csv(\"./outputs/\"+outputname+\"_correlation.csv\")\n",
    "    cor[abs(cor) < 0.8] = 0\n",
    "    plt.imshow(cor, cmap='hot', interpolation='nearest'\n",
    "               ,extent=[wavenumbermin,wavenumbermax,wavenumbermax,wavenumbermin],\n",
    "               vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.savefig(\"./outputs/\"+outputname+\"_correlation.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize=False\n",
    "moremetrices=False\n",
    "\n",
    "normalizepeak=475 #1660\n",
    "\n",
    "print('normalized by:', wavenumber[normalizepeak])\n",
    "\n",
    "\n",
    "MLindex=inputdf['side2'].isin(['right'])& \\\n",
    "        inputdf['adlabel'].isin([0,1])\n",
    "\n",
    "MLdf=inputdf.loc[MLindex]\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(inputdf.iloc[:,10:].loc[MLindex])\n",
    "y=inputdf.iloc[:,:10].loc[MLindex]\n",
    "#set classes\n",
    "column='adlabel'\n",
    "classes=[0,1]\n",
    "\n",
    "def reject_outliers(data,y, m=3):\n",
    "    maximums=np.max(data,axis=1)\n",
    "    index=abs(maximums - np.mean(maximums)) < m * np.std(maximums)\n",
    "    return data[index,:],y[index]\n",
    "#outliers remove\n",
    "X,y=reject_outliers(X,y)\n",
    "X=savgol_filter(X, 7, 2)\n",
    "\n",
    "\n",
    "for i in range(np.shape(X)[0]):\n",
    "    #normalize 1658:475,1439:333\n",
    "    peak=max(X[i,normalizepeak-5:normalizepeak+5])\n",
    "    if peak !=0:\n",
    "        X[i,:]=X[i,:]/max(X[i,normalizepeak-5:normalizepeak+5])\n",
    "\n",
    "print(MLdf,classes,'standardize',standardize)\n",
    "doML(MLdf,column,classes,standardize,moremetrices,max_iter=10000,normalizepeak=normalizepeak)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
